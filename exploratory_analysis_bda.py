# -*- coding: utf-8 -*-
"""Exploratory analysis BDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fBafRukVcIV8HJyEHG8Z693-MErsWVTv
"""

!apt-get install openjdk-8-jdk-headless

# Next, we will install Apache Spark 3.0.1 with Hadoop 2.7 .
!wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz

##Now, we just need to unzip that folder.
!tar xf /content/spark-3.2.1-bin-hadoop2.7.tgz

#There is one last thing that we need to install and that is the findspark library. It will locate Spark on the system and import it as a regular library.

!pip install -q findspark

##Now that we have installed all the necessary dependencies in Colab, it is time to set the environment path. This will enable us to run Pyspark in the Colab environment.

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop2.7"

#We need to locate Spark in the system. For that, we import findspark and use the findspark.init() method.
import findspark
findspark.init()
findspark.find()

#Now, we can import SparkSession from pyspark.sql and create a SparkSession, which is the entry point to Spark.

#You can give a name to the session using appName() and add some configurations with config() if you wish.
from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master("local")\
        .appName("MyFirstEDA")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

#Finally, print the SparkSession variable.
spark

#####importing our datasets
dfad = spark.read.csv("/content/ad-clicks.csv", header=True, inferSchema=True)

dfbuy = spark.read.csv("/content/buy-clicks.csv", header=True, inferSchema=True)

dfgame = spark.read.csv("/content/game-clicks.csv", header=True, inferSchema=True)

df_level= spark.read.csv("/content/level-events.csv", header=True, inferSchema=True)

df_team_assignment = spark.read.csv("/content/team-assignments.csv", header=True, inferSchema=True)

df_team = spark.read.csv("/content/team.csv", header=True, inferSchema=True)

df_user_session = spark.read.csv("/content/user-session.csv", header=True, inferSchema=True)

df_user = spark.read.csv("/content/users.csv", header=True, inferSchema=True)

dfad.printSchema()

dfad.show(5)

dfad.describe()

dfad.describe().toPandas().transpose()

import matplotlib.pyplot as plt
import seaborn as sns

dfad = dfad.groupby("adCategory").count().toPandas()

display(dfad)

##BARPLOT OF CATEGORY OF AD-CLICKED ON
plt.figure(figsize=(10,5))
plt.title('Category of ad clicked On')
sns.barplot(dfad['adCategory'],dfad['count'])

##PIECHART OF CATEGORY OF AD-CLICKED ON
plt.figure(figsize=(15,10))
plt.title('Category of ad clicked On')
plt.pie(dfad['count'],labels=dfad['adCategory'],autopct='%.0f%%')

df_userd =df_user_session.groupby('platformType').count().toPandas()

display(df_userd)

plt.figure(figsize=(10,5))
plt.title('Types of Users Platform')
sns.barplot(df_userd['platformType'],df_userd['count'])

plt.figure(figsize=(15,10))
plt.title('Types of Users Platform')
plt.pie(df_userd['count'],labels=df_userd['platformType'],autopct='%.0f%%')

df_level = df_level.groupby("teamLevel").count().toPandas()

display(df_level)
df_level.hist(column='teamLevel', bins=5, color='r')

dfbuy.printSchema()
dfbuy.limit(3).toPandas()

ranked_dfbuy = dfbuy
ranked_dfbuy.printSchema()

from pyspark.sql.functions import sum, col, desc
df = ranked_dfbuy.groupBy("team") \
  .agg(sum("price").alias("sum_price")) \
  .filter(col("sum_price") > 0)  \
  .sort(desc("sum_price")) \
  .show(10)

ranked_dfbuy.groupBy("team").sum("price").show()

dfGroup=ranked_dfbuy.groupBy("team") \
          .agg(sum("price").alias("sum_price"))

dfSort = dfGroup.sort(desc("sum_price"))

dfsort = dfSort.limit(10).toPandas()

display(dfsort)

plt.figure(figsize=(10,5))
plt.title('Top 10 teams with the most purchase')
sns.barplot(dfsort['team'],dfsort['sum_price']);

####Creating a scatter plot

dfbuy.stat.corr("buyId","price")

df3 = dfbuy.toPandas()
df3.plot.scatter(x='price', y='buyId', c='Green')

pandas_dfbuy = dfbuy.toPandas()

plt.figure(figsize=(10,5))
plt.title('Distribution of price')
#sns.distplot(pandas_dfbuy['price']);
sns.violinplot(pandas_dfbuy['price'], color='pink');

pandas_dfbuy = dfbuy.toPandas()

plt.figure(figsize=(10,5))
plt.title('Distribution of prices')
#sns.distplot(pandas_dfbuy['price']);
sns.boxplot(pandas_dfbuy['price'],color='green');

pandas_dfgame = dfgame.toPandas()

plt.figure(figsize=(10,5))
plt.title('Distribution of price')
sns.distplot(pandas_dfgame['userId']);
#sns.violinplot(pandas_dfbuy['userId']);